# RED Atlas Backend Challenge ‚Äì Senior Level

El objetivo es evaluar tus habilidades en arquitectura, rendimiento, seguridad y mantenibilidad, trabajando sobre un caso realista de gesti√≥n catastral/inmobiliaria.

## Enfoque Estrat√©gico

Este repositorio sigue un **plan de desarrollo por fases** dise√±ado para un challenge t√©cnico:

1.  **Entrega Principal (Estimado 3-4 d√≠as):** Cubre el 100% de los **Core Requirements** para garantizar una evaluaci√≥n exitosa y completa.
2.  **Objetivos Adicionales (Stretch Goals):** Mejoras opcionales (la mayor√≠a de los requisitos **Bonus**) a implementar si la entrega principal se completa antes de la fecha l√≠mite, con el fin de demostrar capacidades avanzadas.

## üìÖ Roadmap de Desarrollo

```
D√≠a 1: üèóÔ∏è Foundation & Setup  ‚Üí  D√≠a 2: üîê Auth & Search  ‚Üí  D√≠a 3: ‚ö° Performance  ‚Üí  D√≠a 4: ‚úÖ Quality & Delivery
```

**Progreso de Core Requirements:**
- **D√≠a 1**: Infraestructura + Modelado + Dataset (25%)
- **D√≠a 2**: Autenticaci√≥n + B√∫squedas + Cache L1 (60%) 
- **D√≠a 3**: PostGIS + Cache L2 + CSV Async (90%)
- **D√≠a 4**: Testing + SLOs + Documentaci√≥n (100%)

[Descripci√≥n del Challenge](docs/challenge-description.md)

[Plan de Soluci√≥n Detallado (por Fases)](docs/plan-of-solution.md)

---

### Notas Importantes

-   **Fecha l√≠mite de entrega:** jueves 21 de agosto.
-   **Ejecuci√≥n:** Debe poder ejecutarse localmente con Docker Compose.
-   **Deploy:** El despliegue p√∫blico no es obligatorio.

## üß™ Plan de Validaci√≥n

**M√©tricas de √âxito Automatizadas:**
- **Performance SLOs**: `autocannon -c 200 -d 60s` ‚Üí p95 ‚â§800ms sin cache, ‚â§300ms con cache
- **Test Coverage**: `npm run test:cov` ‚Üí objetivo >80% coverage
- **Setup Validation**: `docker-compose up` ‚Üí stack completo funcionando en <5min
- **API Documentation**: Swagger UI disponible en `/docs` con 100% endpoints documentados

## üöÄ Instalaci√≥n y Ejecuci√≥n

**Requisitos previos:**
- Docker y Docker Compose instalados
- Git

**Setup completo desde cero:**
```bash
git clone <repo> && cd redatlas-backend
docker compose up -d

# Esperar a que todos los servicios est√©n listos, luego:
docker compose exec api npm run migration:run
docker compose exec api npm run seed
```

**Comandos de Validaci√≥n:**
```bash
# Ejecutar tests (dentro del contenedor)
docker compose exec api pnpm run test:cov

# Validaci√≥n de SLOs de performance
docker compose exec api pnpm run test:load

# Ver logs en tiempo real
docker compose logs -f api

# Acceder al contenedor para debugging
docker compose exec api bash
```

**URLs importantes:**
- **API**: http://localhost:3030
- **Swagger Docs**: http://localhost:3030/docs
- **RabbitMQ Management**: http://localhost:15672 (guest/guest)

## üì¶ Estrategia de Invalidaci√≥n de Cache

### Pol√≠tica de Cache
- **Llaves compuestas**: Cada consulta genera una key basada en par√°metros serializados (JSON hasheado)
- **TTL**: Sin expiraci√≥n autom√°tica, invalidaci√≥n manual por eventos
- **Patr√≥n**: `properties:{hash_de_parametros}`

### Invalidaci√≥n Autom√°tica
El sistema invalida autom√°ticamente el cache en las siguientes operaciones:

**Operaciones que invalidan**:
- `DELETE /properties/:id` - Elimina todo cache `properties:*`
- `POST /imports` - Elimina cache despu√©s de procesar CSV
- Futuras operaciones CUD (CREATE, UPDATE)

**Implementaci√≥n**:
- Interceptor global `CacheInvalidationInterceptor`
- Decorador `@CacheInvalidate` en controladores
- Patr√≥n de invalidaci√≥n: `properties:*` (todos los cache de propiedades)

**Justificaci√≥n**:
- **Consistencia**: Evita datos stale entre cache y DB
- **Simplicidad**: F√°cil mantenimiento vs. invalidaci√≥n granular
- **Performance**: Cache se recrea solo cuando es necesario

### Monitoreo de Cache
```bash
# Ver cache actual
docker compose exec redis redis-cli KEYS "properties:*"

# Limpiar cache manualmente
docker compose exec redis redis-cli FLUSHDB
```

## üß™ Testing Manual

Para probar todas las funcionalidades implementadas, consulta la **[Gu√≠a de Testing Manual](docs/testing-manual.md)** que incluye:

- ‚úÖ Credenciales de prueba (admin@test.com / user@test.com)
- ‚úÖ Setup inicial de base de datos (migraciones ‚Üí seeding)
- ‚úÖ Comandos curl para todas las funcionalidades
- ‚úÖ Verificaci√≥n de cache, b√∫squeda geoespacial e import CSV

---

## üîß Decisiones T√©cnicas

### Arquitectura Docker-First

**Enfoque:** Todo el desarrollo y ejecuci√≥n se realiza dentro de contenedores Docker para garantizar:

- **Consistencia de entorno**: Mismas versiones de Node.js, PostgreSQL, Redis entre desarrolladores
- **Aislamiento de dependencias**: No conflictos con versiones locales instaladas
- **Reproducibilidad**: El entorno de desarrollo es id√©ntico al de producci√≥n
- **Setup simplificado**: Un solo comando `docker compose up -d` para levantar todo el stack

**Importante:** Todos los comandos de desarrollo (`pnpm`, `npm`, tests, migraciones) deben ejecutarse dentro del contenedor usando `docker compose exec api <comando>`.

### Package Manager: pnpm

**Justificaci√≥n:** Seleccionado sobre npm por ventajas t√©cnicas:

- **Performance**: ~2x m√°s r√°pido en instalaciones
- **Eficiencia de espacio**: Symlinks evitan duplicaci√≥n (reducci√≥n ~70% disco)
- **Strict dependency resolution**: Previene phantom dependencies
- **Deterministic installs**: Garantiza reproducibilidad entre entornos
- **Monorepo ready**: Soporte nativo para workspaces

### CSV Import con External ID para UPSERT

**Problema identificado:** Los datos scraped de sitios inmobiliarios contienen direcciones susceptibles a errores de tipeo y inconsistencias de formato, haciendo que la direcci√≥n no sea un identificador √∫nico confiable.

**Soluci√≥n implementada:** Campo `external_id` en el modelo Property para operaciones UPSERT confiables.

**Justificaci√≥n t√©cnica:**
- **Confiabilidad**: external_id generado por el sistema ETL es consistente
- **UPSERT seguro**: Permite actualizar propiedades existentes sin crear duplicados
- **Trazabilidad**: Mantiene referencia al origen del dato en el sistema fuente
- **Escalabilidad**: Facilita sincronizaci√≥n con m√∫ltiples fuentes de datos

**Implementaci√≥n:**
- Campo `external_id` nullable en Property entity (para compatibilidad con datos existentes)
- Validaci√≥n obligatoria en CSV import (external_id requerido)
- L√≥gica UPSERT: INSERT si no existe, UPDATE si ya existe por external_id + tenantId
- Documentaci√≥n actualizada con nuevo formato CSV incluyendo external_id

---

Cuando lo tengas listo, por favor responde al correo con:
1.  Link a tu repositorio (p√∫blico o privado con acceso para nosotros).
2.  Instrucciones claras de instalaci√≥n y ejecuci√≥n (incluidas en este README).
